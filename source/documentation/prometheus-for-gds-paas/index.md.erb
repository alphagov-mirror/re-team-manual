Reliability Engineering operates a monitoring and alerting service for GDS PaaS tenants, and are responsible for the support and reliability of the service.

The service is known informally as "Prometheus for GDS PaaS" and includes [Prometheus][prom-1], [Alertmanager][alerts], [Grafana](https://grafana-paas.cloudapps.digital), supporting infrastructure, metric exporters and user documentation.


## Architecture

[Architectual Drawings](https://www.draw.io/#G1JpBMth6oryBkCKBirF6n51q9yQC6kRIO)

![AWS components](../../images/aws-components.png)
Figure 1: Architecture for components hosted on AWS

### Prometheus

- Three instances of Prometheus are deployed over three AWS
  availability zones in Ireland (eu-west-1) for resilience and high
  availability (figure 1).
- URLs for these instances are:
  - [https://prom-1.monitoring.gds-reliability.engineering/](https://prom-1.monitoring.gds-reliability.engineering/)
  - [https://prom-2.monitoring.gds-reliability.engineering/](https://prom-2.monitoring.gds-reliability.engineering/)
  - [https://prom-3.monitoring.gds-reliability.engineering/](https://prom-3.monitoring.gds-reliability.engineering/)

- Each Prometheus instance has its own persistent EBS storage. Each instance is independent to each other and scrapes metrics separately.
- The three Prometheis are not load balanced and each have their own public URL, routed by the ALB according to the request URL (prom-1, prom-2, prom-3)
- The alerts are configured and generated in Prometheus.
- Each Prometheus sends their alerts to all three of the Alertmanagers.
- Configuration for Prometheus is provided in YAML files which are
  stored in an S3 bucket.
- Changes to the [configuration][prometheus-aws-configuration-beta] are continuously deployed through the [prometheus pipeline][].

[prometheus-aws-configuration-beta]: https://github.com/alphagov/prometheus-aws-configuration-beta

### Alertmanager

<!-- Source: https://hackmd.cloudapps.digital/94zHdlhdQmyGRms_pPFH3A -->
![Architecture](../../images/alertmanager.svg)

Three instances of Alertmanager are deployed over three AWS availability zones in Ireland (eu-west-1) for resilience and high availability.
If you want to browse Alertmanager as a human, go to [GDS Reliability Engineering Alerts Monitoring](https://alerts.monitoring.gds-reliability.engineering/#/alerts).

- You can access each alertmanager separately here:
  - [[alerts-eu-west-1a][]]
  - [[alerts-eu-west-1b][]]
  - [[alerts-eu-west-1c][]]

If you want to configure Prometheus to talk to them, something like the following will work:

```yaml
  alertmanagers:
  - scheme: https
    static_configs:
    - targets:
      - alerts-eu-west-1a.monitoring.gds-reliability.engineering
      - alerts-eu-west-1b.monitoring.gds-reliability.engineering
      - alerts-eu-west-1c.monitoring.gds-reliability.engineering
```

Alertmanager itself is run in ECS Fargate.

- There is one ECS service per AZ, each of which runs a single task.
- Each ECS service registers with a separate per-AZ target group of
  the ALB, and also with a single target group for all alertmanager
  instances.
- For [alertmanager clustering][]:
    - Each alertmanager task needs to talk to each other alertmanager
      task on port 9094
    - We use [ECS Service Discovery][] to automatically register tasks
      on the name `alertmanager.local.gds-reliability.engineering`
    - This automatically creates A records in Route 53 for each task
    - We pass the parameter
      `--cluster.peer=alertmanager.local.gds-reliability.engineering:9094`
      to each alertmanager task to tell it to discover peers using
      this DNS name
- Configuration for Alertmanager is provided in YAML files and
  passed in using an environment variable in the task.  We override
  the `entrypoint` of the container to save the environment variable
  into a file and pass the filename to alertmanager.

Alertmanager is continuously deployed using the [prometheus pipeline][].

[alerts]: https://alerts.monitoring.gds-reliability.engineering/#/alerts
[alerts-eu-west-1a]: https://alerts-eu-west-1a.monitoring.gds-reliability.engineering/#/alerts
[alerts-eu-west-1b]: https://alerts-eu-west-1b.monitoring.gds-reliability.engineering/#/alerts
[alerts-eu-west-1c]: https://alerts-eu-west-1c.monitoring.gds-reliability.engineering/#/alerts
[alertmanager clustering]: https://github.com/prometheus/alertmanager#high-availability
[ECS Service Discovery]: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-discovery.html
[prometheus pipeline]: https://cd.gds-reliability.engineering/teams/autom8/pipelines/prometheus

### Other components

- Grafana: [[Grafana](https://grafana-paas.cloudapps.digital/?orgId=2)]
- One instance of Grafana and a Postgres database (small-9.5) is deployed on GOV.UK PaaS. It uses prom-1 as a data source.

### System Boundary

![System boundary](../../images/system-boundary.png)
Figure 2: System boundary diagram for Prometheus for GDS PaaS - interaction with external systems and services.


### Integration with GDS PaaS applications

![PaaS and Prometheus Interactions](../../images/paas-prometheus.png)

Figure 3: Interaction between PaaS tenants and Prometheus for GDS PaaS and service discovery

- Tenants deploy [Prometheus exporters](#exporter) on PaaS to export container-level, app and service metrics on PaaS with `/metrics` endpoints to be scraped.
- Tenants create a service using the gds-prometheus service broker and bind apps to the service.
- If the tenants wish to restrict the web requests with IP safelist, they can deploy the [ip-safelist route service](#safelist) and bind application routes to the service. This step is optional.
- PaaS tenants can use the Prometheus GUI to query the metrics.
- PaaS tenants can use Grafana to create dashboards for the metrics.
- PaaS tenants can use the PaaS to configure additional targets to be scraped for their organisations.

### Service discovery
- Service discovery allows Prometheus for GDS PaaS to discover which [targets](https://prom-1.monitoring.gds-reliability.engineering/targets) on PaaS to scrape.
- A service broker, named “gds-prometheus”, is available to GDS PaaS tenants (they can see it using `cf marketplace`) and is deployed from the [cf\_app\_discovery](https://github.com/alphagov/cf_app_discovery) code base.
- [cf\_app\_discovery](https://github.com/alphagov/cf_app_discovery) is an app written in Ruby, which is composed of two elements:
  - prometheus-service-broker: a Sinatra app that listens to calls made by the CloudFoundry service API when applications bind to or unbind from the service; and
  - prometheus-target-updater: a cron job that runs every five minutes to tell prometheus-service-broker to detect apps that have been stopped, scaled or killed
- PaaS tenants create a service with the gds-prometheus service broker and bind the apps to the service.  This will register and update the targets to be scraped by Prometheus.
- prometheus-service-broker writes JSON files to an S3 bucket which detail the target to monitor, target labels to use for the target, and the application guid which is used by the instrumentation libraries to protect the /metrics endpoint on the app using basic auth.
- A cron job running on each Prometheus instance syncs these files to the config directory so that Prometheus can pick up the changes.

### AWS Nginx configuration
Nginx is set up in front of Prometheus and acts as an ingress/egress request proxy. It is composed of two elements:

#### paas-proxy:
A forward proxy is used for the traffic from Prometheus to PaaS for two purposes.

- Custom header insertion:
custom headers [X-CF-APP-INSTANCE](https://docs.cloudfoundry.org/concepts/http-routing.html#app-instance-routing), which is a CloudFoundry-specific header which requests a specific instance ID to scrape, is inserted to requests from Prometheus to PaaS so that Prometheus can get metrics from each instance of an app - [EC2 Nginx config](https://github.com/alphagov/prometheus-aws-configuration-beta/blob/master/terraform/modules/prom-ec2/prometheus/cloud.conf#L66).

- Bearer token:
Set to be CloudFoundry app guid, bearer token is used to authorise the connection to the /metrics endpoint for metrics exporters running on PaaS - [EC2 Nginx config](https://github.com/alphagov/prometheus-aws-configuration-beta/blob/master/terraform/modules/prom-ec2/prometheus/cloud.conf#L67).

#### auth-proxy
Basic auth is used to protect inbound access to Prometheus [EC2 Nginx config] (https://github.com/alphagov/prometheus-aws-configuration-beta/blob/master/terraform/modules/prom-ec2/prometheus/cloud.conf#L83-L110), unless the origin of the inbound requests are from office IPs. Basic auth is needed so Grafana, which does not have a static IP, can access Prometheus.


### AWS session manager
We use AWS session manager for accessing AWS node instances using the [systems manager console](https://eu-west-1.console.aws.amazon.com/systems-manager/home?region=eu-west-1#) (login to aws first) or CLI. We do this instead of sshing into the node and do not need a bastion host in our architecture.

### IP safelist for PaaS routes
PaaS tenants can optionally deploy an [IP safelist service](https://docs.cloud.service.gov.uk/deploying_services/route_services/) on PaaS, which is based on [PaaS route service](https://docs.cloud.service.gov.uk/deploying_services/route_services/#route-services) that provides a full proxy for application routes for applications on PaaS, for example [prometheus-metric-exporter](https://github.com/alphagov/paas-prometheus-exporter) that are bound to it. PaaS tenants can use the route service to provide an IP restriction layer before web requests hit the applications running on PaaS.

### Logging, monitoring and alerting
The following apps and SaaS are used for monitoring, logging and alerting for the prometheus-for-PaaS.

#### Logit
We send logs generated from AWS EC2 and PaaS to [Logit](https://reliability-engineering.cloudapps.digital/logging.html#get-started-with-logit), which provides an ELK (Elasticsearch/Logstash/Kibana) service for storing, visualising and filtering logs.

#### Prometheus and Alertmanager
We use Prometheus for GDS PaaS to monitor and alert on itself. Most applications we run expose a /metrics page by default, for example
[Grafana](http://grafana-paas.cloudapps.digital/metrics), or we run additional exporters where needed, for example the [node_exporter](https://github.com/prometheus/node_exporter) on our AWS EC2 instances.

The metrics endpoints we expose for Prometheus for GDS PaaS are summarised in the table below.

|Metric endpoint/ job name|Exposed by|Description|
|--------|----------|-----------|
|[Observe PaaS metrics](https://observe-paas-prometheus-exporter-staging.cloudapps.digital/metrics) {job="observe-paas-prometheus-exporter"}|[paas-prometheus-exporter](#prom-metrics-exporter)|Container-level metrics for apps and services we run in the PaaS|
|[Grafana app metrics](https://grafana-paas.cloudapps.digital/metrics)  {job="grafana-paas"}|Grafana | Application-level metrics for Grafana|
|[Prometheus metrics](https://prom-1.monitoring.gds-reliability.engineering/metrics) {job="prometheus"}|prom-1 on EC2|Prometheus apps metrics. Also available for prom-2 and prom-3|
|[Alertmanager metrics](https://alerts.monitoring.gds-reliability.engineering/metrics) {job="alertmanager"}|alerts on ECS|Alertmanager application metrics.|
|[Service broker](https://prometheus-service-broker.cloudapps.digital/metrics)  {job="prometheus-service-broker"}|[gds_metrics_ruby](#ruby-exporter)|Request metrics for Prometheus service broker|
|EC2 node metrics {job="prometheus_node"}|[node_exporter](https://github.com/prometheus/node_exporter)|VM metrics for the EC2s running Prometheus (The URL is not public)|

You can see available metrics from those metric endpoints either by visiting the endpoint URL (if public facing) or by querying `{job="<job-name>"}` in Prometheus.

#### Cronitor
[Cronitor](https://cronitor.io/) is a “[Dead Man's Switch](https://en.wikipedia.org/wiki/Dead_man%27s_switch)” type of service for health and uptime monitoring of cron jobs. Regular “heartbeats” are sent to Cronitor indicating uptime, it will raise a Pagerduty ticket if it misses the number of heartbeats as configured. We use this to page us if our alerting pipeline is not working.

#### Zendesk and Pagerduty
Zendesk is used for receiving non-interrupting alerts and Pagerduty is used to receive interrupting alerts. Alert priority is defined in the Prometheus alert itself. Alertmanager is used for routing the tickets and pages to the services. The alerting actions and procedures are defined in Zendesk and Pagerduty. Refer to [gds-way](https://gds-way.cloudapps.digital/standards/alerting.html#how-to-manage-alerts) for information on managing alerts.


## Repositories

###Infrastructure, service discovery and secrets

| Repositories | Description |
| -------- | -------- |
| [prometheus-aws-configuration-beta](https://github.com/alphagov/prometheus-aws-configuration-beta)      | Terraform configuration to run Prometheus, Alertmanager and nginx on AWS EC2 and ECS with supporting infrastructure such as S3.    |
| [re-secrets](https://github.com/alphagov/re-secrets) | Contain secrets used for Prometheus for GDS PaaS |
| [cf\_app\_discovery](https://github.com/alphagov/cf_app_discovery)| Cloud Foundry service broker (“gds-prometheus”), that acts as a service discovery agent and updates a list of targets apps to be scraped by Prometheus for GDS PaaS. Tenants bind their apps to the service to be discovered by Prometheus for GDS PaaS.|
| [grafana-paas](https://github.com/alphagov/grafana-paas)|Grafana configured to be deployed to GOV.UK PaaS|

###<a name="exporter">Metric exporters for Prometheus</a>

| Repositories | Description |
| -------- | -------- |
|<a name="prom-metrics-exporter">[paas-prometheus-exporter](https://github.com/alphagov/paas-prometheus-exporter)</a>| Expose container-level app metrics and some backing service metrics for the org that this exporter has read-access to. It reads the metrics from PaaS Doppler component.|
|[gds\_metrics\_dropwizard](https://github.com/alphagov/gds_metrics_dropwizard)|Expose apps metrics for Dropwizard based apps.|
|[gds\_metrics\_python](https://github.com/alphagov/gds_metrics_python)|Expose app metrics for Python based apps|
|<a name="ruby-exporter">[gds\_metrics\_ruby](https://github.com/alphagov/gds_metrics_ruby)</a>|Expose app metrics for Ruby based apps|


###<a name="safelist">IP Safelist proxy service for PaaS services</a>

| Repositories | Description |
| -------- | -------- |
| [paas-ip-authentication-route-service](https://github.com/alphagov/paas-ip-authentication-route-service)     | Cloud foundry route service (an nginx) that implement an IP safelist (whitelist) for Prometheus and GDS office IPs to access /metrics endpoints     |

###Documentation

| Repositories | Description |
| -------- | -------- |
|[reliability-engineering](https://github.com/alphagov/reliability-engineering)|The team maintains the [metrics and logging](https://reliability-engineering.cloudapps.digital/monitoring-alerts.html#metrics-and-alerting) section of the reliability engineering manual. |

## Access infrastructure

### Access to AWS

1. Install the `gds-cli` with `brew install alphagov/gds/gds-cli`.
1. For staging: `gds aws re-prom-staging -l`.
1. For production: `gds aws re-prom-prod -l`.

#### AWS EC2 Access

Access to our EC2 instances is through AWS SSM.

### Access to Paas

Paas information can be found at [Paas](https://docs.cloud.service.gov.uk/).

  - The spaces that are relevant to Observe are:-
    - prometheus-grafana (production grafana)
    - prometheus-production
    - prometheus-staging

### Access to our secrets

Our secrets and passwords are stored within [github](https://github.com/alphagov/re-secrets/tree/master/observe). Our specific secrets are in re-secrets/observe.

### Access restrictions

Prometheus is accessible by office IP safelisting and then falls back to basic auth. Basic auth details are held in [re-secrets](https://github.com/alphagov/re-secrets/tree/master/observe).

Alertmanager is accessible by office IP safelisting but does not fall back to basic auth.

The office IPs can be found here [GDS Internal IP Network](https://sites.google.com/a/digital.cabinet-office.gov.uk/gds-internal-it/news/whitechapel-sourceipaddresses?pli=1).


## Support
### Support hours

The Automate team offer in work hours support, <%= partial 'documentation/prometheus-for-gds-paas/support_times.txt' %>. Any issues out of hours will be recorded and handled during work hours.

### Support process and tasks

- Keep interruptible documentation up to date
- Respond to PagerDuty alerts
- Support users on the `#re-prometheus-support` and `#reliability-eng` slack channels
- Report any problems on the `#re-prometheus-support` and `#reliability-eng` slack channels with regular updates
- Check emails for Logit and PagerDuty status updates
- Check on the status of the Prometheus service
- Check [Zendesk queue](https://govuk.zendesk.com/agent/dashboard) for tickets
- Triage issues and bugs
- Initiate the [incident process](#incident-process)

#### Keep interruptible documentation up to date

Keep the documentation for how to support this service up to date.

#### Respond to PagerDuty alerts

PagerDuty is configured to ring the Interruptible phone when an alert is triggered. PagerDuty alerts should be acknowledged and investigated.

#### Support users on the `#re-prometheus-support` and `#reliability-eng` slack channels

Users of the monitoring service can request help on the `#re-prometheus-support` and `#reliability-eng` slack channels, as interruptible you should be monitoring the channel and engage with users.

- help solve the users problem if it is simple
- [triage](#triage-process) the users problem if it is an issue or bug
- keep the user updated with progress

#### Check emails for Logit and PagerDuty status updates
- inform `#reliability-eng` on slack and send something in the reliability engineering announce email group

#### Check on the status of the Prometheus service

Periodically:

  - Monitor the Prometheus benchmark (beta) dashboard on Grafana
  - Check the Prometheus targets on the active Prometheus dashboard

#### Triage issues and bugs

If you spot what may be an issue or bug then investigate following the [triage process](#triage-process).

### Triage process
One of the goals is to capture which tasks are being performed whilst on the interruptible duty.

It is important that tasks are recorded in [Trello](https://trello.com/b/Z7dOu9Up/re-observe-team) cards so that we understand what tasks are being performed and how long they take, the card should have the label `Interruptible`.
This information is important so that we can feedback and improve the process.

When triaging an issue you should take some time to ask the following questions:

- is someone else already looking at the issue
  - slack the `#re-autom8` channel, ask the team and look at existing Trello cards.
- what impact is it having on tenants:
  - High - does it affect their services, such as causing problems with deployments or affecting performance of their apps.
  - Mid - does it impact their metrics collection, such as seeing unexpected gaps in metrics, odd values, or loss of historical metrics.
  - Low - is it causing problems in viewing metrics on Grafana, but metrics are still being collected and stored.
- how long will the issue take to resolve
  - get estimate from the person who is working on resolving the issue.
  - update the tenant(s) affected with progress.

Ideally you will not need to spend more then 30 minutes finding the answers.

If it is a new issue, and no one else is aware of it then create a [Trello](https://trello.com/b/Z7dOu9Up/re-observe-team) card adding the details you have found and add the appropriate label.

Talk to the team and decide who is going to be responsible for fixing the issue.

### Incident process

- Identify if our users are being affected
- Inform our users on `#re-prometheus-support` for Prometheus, `#reliability-eng` for Logit or team specific channels for issues only affecting a single team.
- If an incident has not already been created on PagerDuty then create one.
- Triage and technical investigation.
- Gather and preserve evidence.
- Resolution, update users that the issue has been resolved.
- Closure, organise a team incident review.

## Reliability

### User expectations

The most important events for our users are:

- their metrics can be successfully scraped by Prometheus
- they can access their dashboards in Grafana
- alerts are delivered to their receivers

This is the minimum so our users can be alerted to problems with their system and debug/monitor them.

We also consider the following important but not as critical for our users:

- access to the Prometheus user interface (most functionality is available in Grafana)
- access to the Alertmanager user interface (currently rarely used by our users)
- alerting rules quickly reviewed and deployed to Prometheus (this is a rare process with little evidence seen so far that quick review and deployment of alerts is essential for users)

Note, these lists may not be exclusive and are expected to change as our system develops.

### Service Level Indicators and Objectives (SLIs and SLOs)

We measure, monitor and alert on our most important user events using SLIs and SLOs. Our SLIs are defined and measured on our [Grafana SLI dashboard](https://grafana-paas.cloudapps.digital/d/wIbJBWbmz/re-observe-slis?orgId=1).

We still need to define and implement SLIs for all of our most important user events (see above). We still need to define SLOs for our SLIs and set up corresponding monitoring and alerting for this. Until these are done we may not find out if we are not meeting our users expecations of our service levels.


## Alerts

### AlwaysAlert

Alert delivery is one of the main things our users rely on. The purpose of this alert is to provide confidence that an alert that fires in Prometheus will be sent from Alertmanager by using a dead mans switch.

This alert is configured to always be firing (so will appear red in Prometheus and as alerting in Alertmanager). The alerts are sent to Cronitor our external monitoring service. If Cronitor has not received an alert from our Alertmanagers for 10 minutes then an alert is raised using Pagerduty.

The Pagerduty alert title will include: RE Observe Heartbeat Production

1. Check using the AWS console that the Prometheus are running in EC2
1. Check the [prometheus deployment pipeline](https://cd.gds-reliability.engineering/teams/autom8/pipelines/prometheus).
1. Check that the alert is firing. [Production](https://prom-1.monitoring.gds-reliability.engineering/alerts) or [Staging](https://prom-1.monitoring-staging.gds-reliability.engineering/alerts)
1. Check using the AWS console that there are sufficient number of running ECS instances (Auto Scaling Group self healing).
1. Check that the alert has arrived at the Alertmanager. [Production](https://alerts.monitoring.gds-reliability.engineering/#/alerts) or [Staging](https://alerts.monitoring-staging.gds-reliability.engineering/#/alerts)
1. If the above are working then slack the `#re-autom8` channel, ask the team to check Cronitor.

#### Links

- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=Dead_Mans_Switch_Constant_Alert)

### RE_Observe_AlertManager_Below_Threshold
#
<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

The current number of Alertmanagers running in production has gone below two.

1. Check using the AWS console that there are sufficient number of running ECS instances (Auto Scaling Group self healing).
2. Check using the AWS console if the ECS Alertmanager tasks are trying to start and are failing to do so.
3. Check the ECS logs for the Alertmanager services - these can be found in the ECS console.

#### Links

- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_AlertManager_Below_Threshold)

### RE_Observe_No_FileSd_Targets

<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

Prometheus has no targets using file service discovery for the GOV.UK PaaS.

It is possible that this is represents a problem with the service broker's
operation that generates the targets for prometheus to scrape, or it may be
that something is preventing the target configurations being fetched and
written to disk, such as a volume mounting failure or insufficient disk space.

Check the [`govukobserve-targets-production` S3 targets bucket](https://s3.console.aws.amazon.com/s3/buckets/govukobserve-targets-production/?region=eu-west-1&tab=overview) in the `gds-prometheus-production` AWS account to ensure that the targets exist in the bucket.

If there are files in the targets bucket then:

1. Check the [Prometheus logs] for errors.
2. SSH onto Prometheus and check if the target files exist on the instance.

If there are no files in the targets bucket then:

1. Check the [service broker logs] for errors.
2. Check the `prometheus-service-broker` and `prometheus-target-updater` are running by logging into the PaaS `prometheus-production` space.

#### Links

- [service broker logs]
- [Prometheus logs]
- [Grafana: Prometheus](https://grafana-paas.cloudapps.digital/d/G-AIv9dmz)
- [Grafana: Service broker](https://grafana-paas.cloudapps.digital/d/JFAHBG1ik)
- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_No_FileSd_Targets)

### RE_Observe_Prometheus_Below_Threshold

<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

The current number of Prometheis running in production has gone below two.

1. Check the status of the Prometheus instances in EC2.
1. Check the [Prometheus logs] for errors.
1. Check the [prometheus deployment pipeline](https://cd.gds-reliability.engineering/teams/autom8/pipelines/prometheus).

#### Links

- [Prometheus logs]
- [Grafana: Prometheus](https://grafana-paas.cloudapps.digital/d/G-AIv9dmz)
- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_Prometheus_Below_Threshold)

### RE_Observe_Prometheus_Disk_Predicted_To_Fill

<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

The available disk space on the `/mnt` EBS volume is predicted to reach 0GB within 72 hours.

Look at [Grafana for the volume's disk usage](https://grafana-paas.cloudapps.digital/d/xIhaZyJmk/prometheus-nodes) or the [raw data in Prometheus](https://prom-3.monitoring.gds-reliability.engineering/graph?g0.range_input=1d&g0.expr=node_filesystem_avail%7B%20mountpoint%3D%22%2Fmnt%22%2C%20job%3D%22prometheus_node%22%20%7D&g0.tab=0&g1.range_input=1d&g1.stacked=0&g1.expr=predict_linear(node_filesystem_avail%7B%20mountpoint%3D%22%2Fmnt%22%20%7D%5B12h%5D%2C3%20*%2086400)&g1.tab=0). This will show the current available disk space.

Increase the EBS volume size (base the increase on the current growth rate in the Prometheus dashboard) in [RE Observe Prometheus terraform repository](https://github.com/alphagov/prometheus-aws-configuration-beta/blob/fc476319f504ee8ede3cefca70fbf9d7137efb7b/terraform/modules/enclave/prometheus/main.tf#L53) code and then run `terraform apply`. When the instance is available `ssh` into each instance, found out which block device is mounted at `/mnt` (`mount -l | grep mnt`) and run `sudo resize2fs /dev/xxx` so that the file system picks up the available disk space.

#### Links

- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_Prometheus_Disk_Predicted_To_Fill)

### RE_Observe_Prometheus_High_Load

<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

Prometheus query engine timing is above the expected threshold. It indicates Prometheus may be beginning to struggle with the current load. This may be caused by:

- too many queries being run against it
- queries being run which are too resource intensive as they query over too many metrics or too long a time period
- an increase in the number of metrics being scraped causing existing queries to be too resource intensive

Queries can originate from a Grafana instance, alerting or recording rules, or be manually run by a user.

If this issue occurs please notify and discuss with the team.

#### Links

- [Grafana: Prometheus](https://grafana-paas.cloudapps.digital/d/G-AIv9dmz)
- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_Prometheus_High_Load)

### RE_Observe_Prometheus_Over_Capacity

<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

Prometheus query engine timing is above the expected threshold. It indicates Prometheus cannot cope with the load and is critically over capacity. This may be caused by:

- too many queries being run against it
- queries being run which are too resource intensive as they query over too many metrics or too long a time period
- an increase in the number of metrics being scraped causing existing queries to be too resource intensive

Queries can originate from a Grafana instance, alerting or recording rules, or be manually run by a user.

If this issue occurs please notify and discuss with the team.

#### Links

- [Grafana: Prometheus](https://grafana-paas.cloudapps.digital/d/G-AIv9dmz)
- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_Prometheus_Over_Capacity)

### RE_Observe_Target_Down

<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

There is a Prometheus target that has been marked as down for 24 hours.

This alert is used as a catch all to identify failing targets that may have no related alert (of which there are several).

You should identify who is responsible for the target and check their alerting rules to see if they would have been notified of this. If they would not have received an alert because they do not have one set up then you should contact them.

If the target is a leftover test app deployed by ourselves then check with the team but we may delete the application if no longer needed or unbind the service from the app, either by [manually running `cf unbind-service`](https://cli.cloudfoundry.org/en-US/cf/unbind-service.html) or by removing the service from the application manifest.

We have also seen a potential bug with our PaaS service discovery leaving targets for
blue-green deployed apps even after the old (also known as venerable) application has been deleted. If this is the case we should try and identify what caused it. If we cannot figure out why, manually delete the file from the [govukobserve-targets-production bucket](https://s3.console.aws.amazon.com/s3/object/govukobserve-targets-production).

#### Links

- [Prometheus targets](https://prom-1.monitoring.gds-reliability.engineering/targets)
- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_Target_Down)

#### Useful commands

Some useful commands that may help when tidying up these targets that are no longer present in the PaaS:

```
# to download targets for Ireland (eu-west-1)
gds aws re-prom-prod -- aws s3 sync s3://govukobserve-targets-production .

# to upload targets for ireland
# note: this command will remove any targets that have been added to the bucket since the above command was run
gds aws re-prom-prod -- aws s3 sync --delete . s3://govukobserve-targets-production

# for London (eu-west-2)
gds aws re-prom-prod -- aws s3 sync s3://govukobserve-london-targets-production .

# note: this command will remove any targets that have been added to the bucket since the above command was run
gds aws re-prom-prod -- aws s3 sync --delete . s3://govukobserve-london-targets-production
```

### RE_Observe_Grafana_Down

<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

The Grafana endpoint has not been successfully scraped for over 5 minutes. This may be caused by:

1. A deploy is taking longer than expected.
2. An issue with the PaaS.

Check:

1. Check with the team to see if there is a current deploy happening.
2. Check the [non 2xx Grafana logs][]

#### Links

- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_Grafana_Down)

### RE_Observe_No_Successful_Updates

<%= partial 'documentation/prometheus-for-gds-paas/support_blurb.md.erb' %>

A region's `gds-prometheus` service broker hasn't successfully performed a target update
in 12 hours. This could mean that something is preventing the process completing in a
particular region. The most notable time that this happened in the past, the London broker
was failing to gather targets due to an authentication issue which persisted for a month
without anyone noticing. Hence this alert.

#### Links

- [service broker logs]
- [Prometheus targets](https://prom-1.monitoring.gds-reliability.engineering/targets)
- [Alert definition](https://github.com/alphagov/prometheus-aws-configuration-beta/search?q=RE_Observe_No_Successful_Updates)

## Runbook

### There is a problem with the monitoring service (Prometheus or Alert Manager)
 - check if the services are available
 - check if there are any deployments in progress
 - check that [Grafana](https://grafana-paas.cloudapps.digital/) is pointing to a live Prometheus service by looking at the data sources under configuration.
 - check the health of the ECS cluster to make sure that the services are running in each AZ.

Escalate the issue to the rest of the team if you are unable to track down the problem.

If the issues are not affecting services  (Users are able to continue to use the service without any disruption) then follow the [triage process](#triage-process).


### There is a problem with one of the Prometheus tenants

Put a message in slack: `#re-prometheus-support` and speak to someone in the team who is responsible for the service which has a problem.


### Adding and editing Grafana permissions

If a user requests a change in Grafana permissions, for example so that they can edit a team dashboard, then you should add that user to the relevant Grafana team and ensure that the team has admin permissions for their team folder.

Do not change a user's overall permissions found in **Configuration > Users** - this should remain as 'Viewer' for all users who are not part of the RE Observe team.

### Rotate basic auth credentials for Prometheus for PaaS

1. Create a new password, for example using `openssl rand -base64 12`

2. Save the plaintext password in the re-secrets store under `re-secrets/observe/prometheus-basic-auth`.

3. Hash the password:

    ```
    docker run -ti ubuntu
    apt-get update
    apt-get -y install whois
    mkpasswd -m sha-512
    ```

4. Append `grafana:` to your hashed password and save this in the re-secrets store under `re-secrets/observe/prometheus-basic-auth-htpasswd`.

5. Deploy Prometheus to staging. As this deploy changes the `cloud.conf` for our instances, you may need to follow steps
in the Prometheus README to deploy with zero downtime.

6. Update the basic auth password for the Prometheus staging data source in Grafana. You will need to do this for every
Grafana organisation.

7. Repeat step 5 for production. Note, as soon as this has been deployed to the main Prometheus that Grafana is using as
a datasource our users dashboards will start breaking as they will still using the old credentials.

8. Repeat step 6 for production.

9. Let users know using the #re-prometheus-support Slack channel that they may need to refresh any Grafana dashboards they
have open to use the new basic auth credentials.


## Architecture history

The major development milestones are summarized as follow:

### Year/Quarter: 2018/Q1

[Alpha architecture diagrams](https://github.com/alphagov/monitoring-doc/tree/master/diagrams)

- Self hosted and configured a prometheus instance on AWS EC2
- Deployed nginx auth-proxy and paas-proxy on the same EC2 machines
- Developed exporters to expose apps and service metrics to be scraped by prometheus
- Developed PaaS service-broker for the exporters for PaaS tenants to export their metrics to Prometheus

### Year/Quarter: 2018/Q2-3

[Beta architecture diagrams](https://docs.google.com/document/d/1FFT6lqOknXNYfGYptTJ8E-8LPdroN7jvTQACSjjBUtU/edit#heading=h.iznzu7xflj1)

- Deploy 3 instances of Prometheus on AWS ECS
- Deployed 3 instances of Alertmanager on AWS ECS
- Deployed 1 instances of Grafana on GOV.UK PaaS
- Configure metrics and logs monitoring for the service
- Later migrate Prometheus and nginx processes from ECS to EC2
- Successfully tested 2 instances of Alertmanager running on the new Kubernetes platform
- Started migration of Nginx auth-proxy and paas-proxy back from ECS to EC2

### Year/Quarter: 2019/Q4

- Fixed meshing across the Alertmanagers
- Migrated Alertmanagers to Fargate
- Made Alertmanagers continously deployed through Concourse



[Prometheus logs]: https://kibana.logit.io/s/8fd50110-7b0c-490a-bedf-7544daebbec4/app/kibana#/discover
[service broker logs]: https://kibana.logit.io/s/8fd50110-7b0c-490a-bedf-7544daebbec4/app/kibana#/discover
